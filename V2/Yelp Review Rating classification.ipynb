{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/SD'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/SD/Downloads\n"
     ]
    }
   ],
   "source": [
    "cd Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/SD/Downloads/yelp_dataset_challenge_academic_dataset\n"
     ]
    }
   ],
   "source": [
    "cd yelp_dataset_challenge_academic_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "reviews = []\n",
    "i = 0\n",
    "with open('yelp_academic_dataset_review.json') as f:\n",
    "    for line in f:\n",
    "        reviews.append(json.loads(line))\n",
    "        if i == 250000:\n",
    "            with open('subset-reviews-0.pkl','w') as fw:\n",
    "                pickle.dump(reviews,fw)\n",
    "                break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5160856\r\n",
      "-rw-r-----@ 1 SD  staff      397375 Jan 13 11:45 Dataset_Challenge_Academic_Dataset_Agreement.pdf\r\n",
      "-rw-r-----@ 1 SD  staff      238869 Jan 13 11:45 Yelp_Dataset_Challenge_Terms_round_7.pdf\r\n",
      "-rw-r--r--  1 SD  staff   251301066 Apr 24 14:11 subset-reviews-0.pkl\r\n",
      "-rw-r--r--@ 1 SD  staff    69035997 Jan  7 20:34 yelp_academic_dataset_business.json\r\n",
      "-rw-r--r--@ 1 SD  staff    25516317 Jan  7 20:38 yelp_academic_dataset_checkin.json\r\n",
      "-rw-r--r--@ 1 SD  staff  1940765746 Jan  7 20:37 yelp_academic_dataset_review.json\r\n",
      "-rw-r--r--@ 1 SD  staff   118706610 Jan  7 20:38 yelp_academic_dataset_tip.json\r\n",
      "-rw-r--r--@ 1 SD  staff   236377074 Jan  7 20:38 yelp_academic_dataset_user.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'business_id': u'5UmKMjUEUNdYWqANhGckJw',\n",
      " u'date': u'2012-08-01',\n",
      " u'review_id': u'Ya85v4eqdd6k9Od8HbQjyA',\n",
      " u'stars': 4,\n",
      " u'text': u'Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.',\n",
      " u'type': u'review',\n",
      " u'user_id': u'PUFPaY9KxDAcGqfsorJp3Q',\n",
      " u'votes': {u'cool': 0, u'funny': 0, u'useful': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, listname):\n",
    "        self.listname = listname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for review in self.listname:\n",
    "            clean_review = clean_str(review['text'])\n",
    "            yield LabeledSentence(words=clean_review.split(), tags=str(review['stars']))\n",
    "            \n",
    "sentences = MySentences(reviews[:250]) # a memory-friendly iterator\n",
    "model = gensim.models.Doc2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'no', 0.8778679966926575),\n",
       " (u'maybe', 0.8742163777351379),\n",
       " (u'garlic', 0.8704522252082825),\n",
       " (u'peppers', 0.867580235004425),\n",
       " (u'2', 0.8627973198890686),\n",
       " (u'decent', 0.8350401520729065),\n",
       " (u'two', 0.8230847120285034),\n",
       " (u'eggs', 0.8163115978240967),\n",
       " (u'don', 0.8115946650505066),\n",
       " (u'italian', 0.8049217462539673)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39081"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
